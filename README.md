# Simple MLP Using NumPy

## Overview
This project implements a **Multi-Layer Perceptron (MLP)** from scratch using **NumPy**, without relying on deep learning frameworks like TensorFlow or PyTorch. It serves as an educational example to understand the inner workings of neural networks, including forward propagation, backpropagation, and optimization.

## Features
- Fully connected feedforward neural network.
- Implements **forward and backward propagation** manually.
- Uses **Mean Squared Error (MSE) or Cross-Entropy Loss** for optimization.
- Supports training on synthetic datasets.
- Adjustable **learning rate, activation functions**, and **number of layers**.

## Dependencies
- Python 3.x
- NumPy

## How to Run
1. Clone the repository:
   ```sh
   git clone https://github.com/ydv1412/A-Simple-MLP-Using-Numpy.git
   cd A-Simple-MLP-Using-Numpy
